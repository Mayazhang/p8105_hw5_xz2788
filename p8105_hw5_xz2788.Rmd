---
title: "p8105_hw5_xz2788"
author: "Xiaoyue Zhang"
date: "11/2/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load the necessary package

```{r load_package}
library(tidyverse)
```

## Problem 1

First read in all file names into a dataframe

```{r filename}
longidata = tibble(
  file_name = list.files(path = "./data")
)
```

Add in the data of each file to the dataframe and unnest the list

```{r file_data, message=FALSE}
longidata$file = str_c('./data/',longidata$file_name)
longidata$data = map(longidata$file, read_csv)
longidata = unnest(longidata)
```

Tidy the dataset

```{r tidy_data}
longidata$file_name = str_replace(longidata$file_name, ".csv", "")
longidata = longidata %>% 
  select(-file) %>% 
  separate(file_name, into = c("arm", "subject_id"), sep = "_") %>% 
  gather(key = week, value = observation, week_1:week_8)
longidata$week = str_replace(longidata$week, "week_", "") %>% as.numeric()
longidata$arm = str_replace(longidata$arm, "con", "control")
longidata$arm = str_replace(longidata$arm, "exp", "experimental")
```

Check the resulting dataset to make sure it's tidy

```{r check_dataset}
head(longidata)
str(longidata)
```

Make the required spaghetti plot

```{r spaghetti_plot}
longidata$arm_id = str_c(longidata$arm, '_', longidata$subject_id)
#combine the "subject_id" and "arm" to make a new variable for later plotting

longidata %>% 
  ggplot(aes(x = week, y = observation, group = arm_id, color = arm)) +
  geom_line() +
  theme_bw() +
  labs(
    title = "Observations of each subject through time",
    x = "Week",
    y = "Observation data",
    caption = "Data from the longitudinal study"
  ) +
  theme(legend.position = "bottom")
```

Comment : From the sphaghetti plot, the observation data of experimental group are roughly higher than that of control group through 8 weeks and the difference between two groups became larger when time went by. However, we still need to consider the comparability of two groups at the beginning of the experiment and this can't be judged just by this plot.

## Problem 2

Read in the raw data of homicides and parse columns

```{r read_data, message=FALSE, warning=FALSE}
homicide = read_csv(file = "./homicide-data.csv",
                    col_types = "cccccdcccddc")
```

### Description of raw data:
The "homicide" dataset in total have `r nrow(homicide)` observations and `r ncol(homicide)` variables. Each observation is a homicide. Information includes the age, sex, name, race of the victim, date, city, state and location of the homicides as well as whether the homicide was closed. Except that age and location are numeric variables, others are character variables. Cases were counted as closed without arrest if they were reported by police to be “exceptionally cleared.”

Create a "city_state" variable

```{r create_variable}
homicide$city_state = str_c(homicide$city, ", ", homicide$state)
```

### Summarize within cities to get the total number of homicides 

(here using "city" variable to group_by, because the number of distinct cities is 50 which matches the total number of cities and this means there are no any two cities having the same name)

```{r total_homicides}
homicide %>% 
  group_by(city) %>% 
  summarize(n_homicide = n())
```

### Summarize within cities to get the unsolved cases

```{r num_unsolved}
homicide %>% 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>%
  group_by(city) %>% 
  summarize(n_unsolved = n())
```

### For Baltimore

```{r esti_prop}
homicide %>% 
  filter(city == "Baltimore") %>% 
  group_by(disposition) %>% 
  summarize(n = n()) 

baltimore = prop.test(x = 1825, n = 2827)  
```

From the summary, there were in total `r 1002 + 152 + 1673` homicides in Baltimore and `r 152 + 1673` were unsolved. Thus I used these two numbers as "x" and "n" to estimate the proportion.

```{r broom_tidy}
broom::tidy(baltimore) %>% 
  janitor::clean_names() %>% 
  select(estimate, conf_low, conf_high)
```

Therefore, the estimated proportion of unsolved homicides in Baltimore is 64.56% and confidence interval is (0.6276, 0.6632).

### Estimate proportion in each city

First calculate the unsolved homicides and total homicides in each city

```{r prop_city}
unsolved = homicide %>% 
  group_by(city, disposition) %>% 
  summarize(n = n()) %>% 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>% 
  group_by(city) %>% 
  summarize(n_unsolved = sum(n))

total = homicide %>% 
  group_by(city) %>% 
  summarize(total = n())
```

Creat a function for prop_test in each city

```{r prop_function}
tidy_prop_test = function(x, n) {
  
  prop.test(x, n) %>% 
    broom::tidy() %>% 
    janitor::clean_names() %>% 
    select(estimate, conf_low, conf_high)
  
}
```

Creat a dataframe for estimating proportion and confidence interval for each city

```{r result_of_prop}
prop_results = unsolved %>% 
  mutate(model = map2(.x = unsolved$n_unsolved, 
     .y = total$total, 
     ~tidy_prop_test(x = .x, n = .y))) %>% 
  unnest() %>% 
  select(-n_unsolved)
```

### Make a plot to show estimates and CIs for each city

```{r plot_for_cities}
prop_results %>% 
  transform(city = reorder(city, estimate)) %>% 
  ggplot(aes(x = city, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Estimated proportion of unsolved homicides and CI",
    x = "City",
    y = "Estimated proportion",
    caption = "Data from the Washington Post"
  )
```

